{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression Using Gradient Descent \n",
    "\n",
    "In this lab, you will be implementing the Linear Regression Model on the same data set as in Lab_1a , but here we will be using the gradient descent algorithm (GDA) and the stochastic gradient descent algorithm (SGDA) to minimize the cost function as we covered in the class.\n",
    "\n",
    "Please add your own print statements to check your code to ensure your code is correct in every step.  (Note: we will not be grading the print statements you add to your code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this code cell using shift+enter before moving further\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data set.\n",
    "\n",
    "In the below code cell, you will load the data using python pandas library as done in the programming_assignment_1a. Use pd.read_csv('File url ', header=None,.... ) with the value of header=None,delim_whitespace=True,names=names,na_values='?' as attributes. The url for the .data file is https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data. This step is same as done in programming_assignment_1a.  You could have found the names by inspecting the dataset's metadata, at https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further.\n",
    "names =[\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n",
    "    'AGE',  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE'\n",
    "]\n",
    "\n",
    "#Write your code below to save dataframe in the df variable below. \n",
    "# In place of None, write the pandas command to read the csv file.\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data',header = None,delim_whitespace=True, names = names, na_values='?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the values from the 'PRICE' and 'RM' columns. into a smaller dataframe named df1 from df. This step is the same as was done in programming_assignment_1a.  Then drop rows with NaN values from df1, and save the result in df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 2)\n"
     ]
    }
   ],
   "source": [
    "#  After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below.\n",
    "df1= df[['PRICE','RM']]\n",
    "df2= df1.dropna()\n",
    "print (df2.shape)\n",
    "# Check the shape of df2. It should be (506,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector y having the values of 'PRICE' column and vector x having the values of 'RM' column. This step is also same as done in programming_assignment_1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below.\n",
    "x= df['RM']\n",
    "y= df['PRICE']\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "# Check the shape of x and y vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape x and y to be rank 2 (here we refer to the number of dimentions of a array as it's rank). After checking the shape of x and y in the above code cell, we see that x and y are rank 1 matricies. Before proceeding,  convert them to be rank 2 matricies. For example, you could use the command x=x.reshape(x.shape[0],1) to reshape x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below\n",
    "x=x.reshape(x.shape[0],1)\n",
    "y=y.reshape(x.shape[0],1)\n",
    "print(x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data.\n",
    "Plot the dataset by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJJyqAgIISTlUBEQQkjKoSIghJCU8/8BsjlhrWebukwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114371b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just run this code cell.\n",
    "%matplotlib inline\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel(\"RM\")\n",
    "plt.ylabel('PRICE')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the value of n, i.e. number of training examples. \n",
    "Hint: Value of n is equal to the number of rows in either x or y matrix which can be accessed using the numpy shape command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n"
     ]
    }
   ],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below\n",
    "n= x.shape[0]\n",
    "print(n)\n",
    "# After writing a code, it is a good practice to verify that your code is correct. \n",
    "# For example, in this case you can print the value of n and check that it should be equal to 506.\n",
    "# Occasionally we created test code cell to verify your code. \n",
    "# However, you should do it for almost every code you write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost: Write the code to compute the cost inside the function. Do not change the function name or function parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(x, y, beta0, beta1, n):\n",
    "    #Write your code in place of None. Cost can be calculated using a single line of code\n",
    "    cost= np.sum((y - (beta0 + beta1*x))**2) / (2*n)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that the code you have written to compute the cost is correct. Just run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296.073458498\n"
     ]
    }
   ],
   "source": [
    "cost_verify= compute_cost(x, y, 0, 0, n)\n",
    "\n",
    "print(cost_verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be equal to 296.073458498.  Ensure your answer is correct before continuing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Write the code to perform gradient descent in the function below.  In this assigment, we will run the algorithm a fixed number of iterations.  Later in the course, we will add a simple extension by adding a stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, learning_rate, beta0, beta1, n, num_iters):\n",
    "    # In place of None, write the updated value of beta0 in temp0 and of beta1 in temp1\n",
    "    for i in range(num_iters):\n",
    "        temp0 =   beta0 - np.sum((learning_rate) * (beta0 + beta1*x - y))\n",
    "        temp1 =   beta1 - np.sum(((learning_rate) * (beta0 + beta1*x - y)*x))\n",
    "        beta0 = temp0\n",
    "        beta1 = temp1\n",
    "        \n",
    "        \n",
    "        # In place of None, call the cost you just coded above\n",
    "        cost= compute_cost(x, y, beta0, beta1, n)\n",
    "        #print(\"Step \" + str(i+1) + \": \" + \" beta 0: \" + str(beta0) + \"beta 1: \" + str(beta1) )\n",
    "        print(beta0,beta1)\n",
    "    \n",
    "    \n",
    "    return beta0,beta1       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that your code to update beta0 and beta1 is correct. Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5 -6.0\n",
      "9.75 39.0\n",
      "-74.625 -298.5\n",
      "558.1875 2232.75\n"
     ]
    }
   ],
   "source": [
    "g=gradient_descent(4, -3, 0.5, 0, 0, n, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last output should be: -34.3837376153. Ensure that you have the correct result before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integerating the Batch Gradient Descent Function \n",
    "\n",
    "Using the above code, create a single function linear_reg_model_gda: This function uses the gradient descent algorithm to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_reg_model_gda(x, y, n, learning_rate, num_iters):\n",
    "    #initialize the values of parameters beta0 and beta1 both to 0\n",
    "    beta0=0\n",
    "    beta1=0\n",
    "    \n",
    "    #calculate the initial cost by calling the function you just coded above\n",
    "    initial_cost=compute_cost(x,y,beta0,beta1,n)\n",
    "    print(\"Initial Cost\")\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of beta0 and beta1 by calling the gradient_descent function coded above\n",
    "    \n",
    "    beta0,beta1= gradient_descent(x, y, 0.04, 0, 0, n, 10000)\n",
    "    \n",
    "    #Calculate the cost with the optimized value of beta0 and beta1 by calling the cost function.\n",
    "    \n",
    "    final_cost=compute_cost(x,y,beta0,beta1,n)\n",
    "    print(\"Final Cost\")\n",
    "    print(final_cost)\n",
    "    return beta0,beta1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have completed the linear_reg_model_gda function, you can use this function to find the optimized values of beta0 and beta1. Using it, set the values of learning_rate and num_iters to something reasonable. You may have to call this function several times with different values of num_iters and learning_rate to find the optimal values of beta0 and beta1. For some values of learning_rate, you may recieve incorrect values of beta0 and beta1, wherein they reach a very large value(infinity) due to overshooting as was discussed in class. Finally, the values of beta0, beta1 and cost(RSS) should be same(or very close to) the ones in programming_assignment_1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost\n",
      "296.073458498\n",
      "Cost\n",
      "294.211536124\n",
      "Betas\n",
      "0.00189723320158\n",
      "0.0124743083004\n",
      "Cost\n",
      "170.025517407\n",
      "Betas\n",
      "0.153799541417\n",
      "0.975955794892\n",
      "Cost\n",
      "95.2964785109\n",
      "Betas\n",
      "0.278409052376\n",
      "1.79315235378\n",
      "Cost\n",
      "48.7132755213\n",
      "Betas\n",
      "0.393079528597\n",
      "2.60810503928\n",
      "Cost\n",
      "42.163338681\n",
      "Betas\n",
      "0.421731899583\n",
      "2.78769133929\n",
      "Cost\n",
      "44.3301837746\n",
      "Betas\n",
      "0.411480358144\n",
      "2.72404314364\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 506 is out of bounds for axis 0 with size 506",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-9727bbb73d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# In place of None, call the linear_reg_model_gda.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_reg_model_gda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-302-0da8605d1f30>\u001b[0m in \u001b[0;36mlinear_reg_model_gda\u001b[0;34m(x, y, n, learning_rate, num_iters)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#calculate the optimized value of beta0 and beta1 by calling the gradient_descent function coded above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#Calculate the cost with the optimized value of beta0 and beta1 by calling the cost function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-89e90d99ccad>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(x, y, learning_rate, beta0, beta1, n, num_iters)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# In place of None, write the updated value of beta0 in temp0 and of beta1 in temp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtemp0\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mbeta0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtemp1\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mbeta1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbeta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 506 is out of bounds for axis 0 with size 506"
     ]
    }
   ],
   "source": [
    "# Write your code below\n",
    "learning_rate=0.04\n",
    "num_iters=Non=10000\n",
    "# In place of None, call the linear_reg_model_gda.\n",
    "beta0,beta1 = linear_reg_model_gda(x, y, n, learning_rate, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Price of a House"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your function to train your linear regression model to find the optimal values for $\\beta_0$ and $\\beta_1$.  Once you have the optimal values for the parameters, you can predict the value of $y$ (price) using $x$ (rm).  Compute the function below to prdict $y$, using $x$, $\\beta_0$ and $\\beta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, beta0, beta1):\n",
    "    predicted_y = beta0 = beta1*x\n",
    "    \n",
    "    return predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, predict the price of a house with rm=6 using ```predict()```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.19248830e+09]\n"
     ]
    }
   ],
   "source": [
    "# Call the predict function with rm=6 \n",
    "y_predict = predict(6, beta0, beta1)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value should be approximately 19.54424 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation Method\n",
    "Now, we will be writing the code to find the values of parameters beta0 and beta1 for our linear regression model. This can also be used to cross-check the optimal values of beta0 and beta1 we just found above using the above two models. These values should be approximately the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Appending a column of ones to x in the left. Save this into X. You can first create a column vector of ones say 'a' (ensure this to have dimension (n,1) i.e. a rank 2 array). Now, you can use np.hstack to append it to the left of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the code below\n",
    "a = np.ones((506,1))\n",
    "X = np.hstack((a,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Instead of writing the code for normal equation in one line, you can break this into 3 parts: First calculate q=inverse of (dot of (X.T,X)) (these are pseudo commands, use original numpy commands to calculate q). Then w= dot of ( X.T , y) and then beta_vec= dot of (q,w). Here, beta_vec is vector of dimension (2,1) having two values. Example beta0=beta_vec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the code below\n",
    "q = np.dot(X.T,X)\n",
    "w = np.dot(X.T,y)\n",
    "beta_vec = np.dot(q,w)\n",
    "beta0 = beta_vec[0]\n",
    "beta1 = beta_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta0 is [  2.40849624e+08]\n",
      "Beta1 is [  1.53208138e+09]\n"
     ]
    }
   ],
   "source": [
    "print(\"Beta0 is \" + str(beta0))\n",
    "print(\"Beta1 is \" + str(beta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Values of beta0 and beta1 you just got above should be approximately the same as the ones you got using linear_reg_model_gda or linear_reg_model_sgda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optional (The following will not be graded):  \n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "You can read more about stochastic gradient descent: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "To prevent cycles, shuffle the data for each pass.\n",
    "\n",
    "Write the code to perform a stochastic gradient descent. Remember, every update in a sgda uses examples one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, learning_rate, beta0, beta1, n, num_iters):\n",
    "\n",
    "    for j in range(num_iters):\n",
    "    \n",
    "        for i in range(0,n):\n",
    "        # Write updated value in beta0 in temp0 and of beta1 in temp1\n",
    "            temp0 = None \n",
    "            temp1 = None\n",
    "            beta0 = temp0\n",
    "            beta1 = temp1   \n",
    "\n",
    "        if(j%2000==0):\n",
    "            cost= compute_cost(x,y,beta0,beta1,n)\n",
    "            print(\"Cost\")\n",
    "            print(cost)\n",
    "            print(\"Betas\")\n",
    "            print(beta0)\n",
    "            print(beta1)           \n",
    "            \n",
    "    return beta0,beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that your code to update beta0 and beta1 is correct. Run the code cell below. This may take some time to run completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=stochastic_gradient_descent(x, y, 0.0048, 0, 0, n, 10000)\n",
    "print(g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be: -17.98968896 . Ensure that your answer is correct before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating the Stochastic Gradient Descent Algorithm\n",
    "\n",
    "Use this function to complete linear_reg_model_sgda(). This function uses stochastic gradient descent to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_reg_model_sgda(x, y, n, learning_rate, num_iters):\n",
    "    \n",
    "     #initialize the values of parameters beta0 and beta1 both to 0\n",
    "    beta0=0\n",
    "    beta1=0\n",
    "    \n",
    "    #calculate the initial cost by calling the function cost you just coded above\n",
    "    print(\"Initial Cost\")\n",
    "    initial_cost= compute_cost(x,y,beta0,beta1,n)\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of beta0 and beta1 by calling the stochastic_gradient_descent function coded above\n",
    "    \n",
    "    beta0,beta1= stochastic_gradient_descent(x, y, learning_rate, beta0, beta1, n, num_iters)\n",
    "    \n",
    "    #Calculate the cost with the optimized value of beta0 and beta1 by calling the cost function.\n",
    "    \n",
    "    final_cost=compute_cost(x,y,beta0,beta1,n)\n",
    "    print(\"Final_cost\")\n",
    "    print(final_cost)\n",
    "    return beta0,beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have completed linear_reg_model_sgda function, you can call this function to find the optimized values of beta0 and beta1. Before calling the function, set the values of learning_rate and num_iters appropriately. You may have to call this function several times with different values of num_iters and learning_rate to find the optimal values of beta0 and beta1. For a sufficiently high learning_rate, it may return extremely high values for beta0 and beta1 (infinity). Finally, the values of beta0, beta1 and cost(RSS) should be same(or nearly the same) as you got in the programming_assignment1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "learning_rate = 0.045\n",
    "num_iters = 10000\n",
    "# In place of None call the function linear_reg_model_sgda.\n",
    "beta0,beta1 = linear_reg_model_sgda(x, y, n, learning_rate, num_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
